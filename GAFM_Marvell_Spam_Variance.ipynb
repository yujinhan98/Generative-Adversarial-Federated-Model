{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yujin/.conda/envs/yjenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from abc import ABCMeta, abstractmethod\n",
    "class AbstractAttacker(metaclass=ABCMeta):\n",
    "    def __init__(self, splitnn):\n",
    "        \"\"\"attacker against SplitNN\n",
    "        Args:\n",
    "            splitnn: SplitNN\n",
    "        \"\"\"\n",
    "        self.splitnn = splitnn\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def attack(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NormAttack(AbstractAttacker):\n",
    "    def __init__(self, splitnn):\n",
    "        \"\"\"Class that implement normattack\n",
    "        Args:\n",
    "            splitnn (attack_splitnn.splitnn.SplitNN): target splotnn model\n",
    "        \"\"\"\n",
    "        super().__init__(splitnn)\n",
    "        self.splitnn = splitnn\n",
    "\n",
    "    def attack(self, dataloader, criterion, device):\n",
    "        \"\"\"Culculate leak_auc on the given SplitNN model\n",
    "           reference: https://arxiv.org/abs/2102.08504\n",
    "        Args:\n",
    "            dataloader (torch dataloader): dataloader for evaluation\n",
    "            criterion: loss function for training\n",
    "            device: cpu or GPU\n",
    "        Returns:\n",
    "            score: culculated leak auc\n",
    "        \"\"\"\n",
    "        epoch_labels = []\n",
    "        epoch_g_norm = []\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = self.splitnn(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.splitnn.backward()\n",
    "\n",
    "            grad_from_server = self.splitnn.client.grad_from_server\n",
    "            g_norm = grad_from_server.pow(2).sum(dim=1).sqrt()\n",
    "            epoch_labels.append(labels)\n",
    "            epoch_g_norm.append(g_norm)\n",
    "\n",
    "        epoch_labels = torch.cat(epoch_labels)\n",
    "        epoch_g_norm = torch.cat(epoch_g_norm)\n",
    "        score = roc_auc_score(epoch_labels, epoch_g_norm.view(-1, 1))\n",
    "        return score\n",
    "class DataSet(Dataset):\n",
    "    \"\"\"This class allows you to convert numpy.array to torch.Dataset\n",
    "    Args:\n",
    "        x (np.array):\n",
    "        y (np.array):\n",
    "        transform (torch.transform):\n",
    "    Attriutes\n",
    "        x (np.array):\n",
    "        y (np.array):\n",
    "        transform (torch.transform):\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"get the number of rows of self.x\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "def torch_roc_auc_score(label, pred):\n",
    "    return roc_auc_score(label.cpu().detach().numpy(),\n",
    "                         pred.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 4601\n",
      "    Positive: 1813 (39.40% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n",
    "  \n",
    "import pandas as pd\n",
    "raw_df = pd.concat([X, y], axis=1)\n",
    "raw_df = raw_df.rename(columns={raw_df.columns[-1]: 'label'})\n",
    "scaler = preprocessing.StandardScaler()\n",
    "raw_df.iloc[:,:-1] = pd.DataFrame(scaler.fit_transform(raw_df.iloc[:,:-1]), columns = raw_df.iloc[:,:-1].columns)\n",
    "raw_df_neg = raw_df[raw_df[\"label\"] == 0]\n",
    "raw_df_pos = raw_df[raw_df[\"label\"] == 1]\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    raw_df.shape[0],raw_df_pos.shape[0], 100 * raw_df_pos.shape[0] / raw_df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Training labels shape: (3220,)\n",
      "Test labels shape: (1381,)\n",
      "Training features shape: (3220, 57)\n",
      "Test features shape: (1381, 57)\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"batch_size\":1028\n",
    "}\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_df, test_df = train_test_split(raw_df, test_size=0.3)\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df['label'])\n",
    "bool_train_labels = train_labels != 0\n",
    "# val_labels = np.array(val_df)\n",
    "test_labels = np.array(test_df['label'])\n",
    "\n",
    "train_features = np.array(train_df.drop(['label'],axis=1))\n",
    "# val_features = np.array(val_df)\n",
    "test_features = np.array(test_df.drop(['label'],axis=1))\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "# print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "# print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)\n",
    "train_dataset = DataSet(train_features,\n",
    "                        train_labels.astype(np.float64).reshape(-1, 1))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=config[\"batch_size\"],\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = DataSet(test_features,\n",
    "                       test_labels.astype(np.float64).reshape(-1, 1))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstNet(nn.Module):\n",
    "    def __init__(self,hidden_dim = 10):\n",
    "        super(FirstNet, self).__init__()        \n",
    "        self.L1 = nn.Linear(train_features.shape[-1],\n",
    "                            hidden_dim)\n",
    "        self.L2 = nn.Linear(hidden_dim,\n",
    "                            1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.L1(x)\n",
    "        x = nn.functional.leaky_relu(x)\n",
    "        x = self.L2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        # x = nn.functional.leaky_relu(x)\n",
    "        return x\n",
    "    \n",
    "class SecondNet(nn.Module):\n",
    "    def __init__(self,hidden_dim = 10):\n",
    "        super(SecondNet, self).__init__()        \n",
    "        self.L1 = nn.Linear(1,\n",
    "                            hidden_dim)\n",
    "        self.L2 = nn.Linear(hidden_dim,\n",
    "                            1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.L1(x)\n",
    "        x = nn.functional.leaky_relu(x)\n",
    "        x = self.L2(x)\n",
    "        # x = nn.functional.leaky_relu(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def torch_auc(label, pred):\n",
    "    return roc_auc_score(label.cpu() .detach().numpy(),\n",
    "                         pred.cpu() .detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SplitNN\n",
    "import torch\n",
    "class Client(torch.nn.Module):\n",
    "    def __init__(self, client_model):\n",
    "        super().__init__()\n",
    "        \"\"\"class that expresses the Client on SplitNN\n",
    "        Args:\n",
    "            client_model (torch model): client-side model\n",
    "        Attributes:\n",
    "            client_model (torch model): cliet-side model\n",
    "            client_side_intermidiate (torch.Tensor): output of\n",
    "                                                     client-side model\n",
    "            grad_from_server\n",
    "        \"\"\"\n",
    "\n",
    "        self.client_model = client_model\n",
    "        self.client_side_intermidiate = None\n",
    "        self.grad_from_server = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"client-side feed forward network\n",
    "        Args:\n",
    "            inputs (torch.Tensor): the input data\n",
    "        Returns:\n",
    "            intermidiate_to_server (torch.Tensor): the output of client-side\n",
    "                                                   model which the client sent\n",
    "                                                   to the server\n",
    "        \"\"\"\n",
    "\n",
    "        self.client_side_intermidiate = self.client_model(inputs)\n",
    "        # send intermidiate tensor to the server\n",
    "        intermidiate_to_server = self.client_side_intermidiate.detach()\\\n",
    "            .requires_grad_()\n",
    "\n",
    "        return intermidiate_to_server\n",
    "\n",
    "    def client_backward(self, grad_from_server):\n",
    "        \"\"\"client-side back propagation\n",
    "        Args:\n",
    "            grad_from_server: gradient which the server send to the client\n",
    "        \"\"\"\n",
    "        self.grad_from_server = grad_from_server\n",
    "        self.client_side_intermidiate.backward(grad_from_server)\n",
    "\n",
    "    def train(self):\n",
    "        self.client_model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.client_model.eval()\n",
    "\n",
    "\n",
    "class Server(torch.nn.Module):\n",
    "    def __init__(self, server_model):\n",
    "        super().__init__()\n",
    "        \"\"\"class that expresses the Server on SplitNN\n",
    "        Args:\n",
    "            server_model (torch model): server-side model\n",
    "        Attributes:\n",
    "            server_model (torch model): server-side model\n",
    "            intermidiate_to_server:\n",
    "            grad_to_client\n",
    "        \"\"\"\n",
    "        self.server_model = server_model\n",
    "\n",
    "        self.intermidiate_to_server = None\n",
    "        self.grad_to_client = None\n",
    "\n",
    "    def forward(self, intermidiate_to_server):\n",
    "        \"\"\"server-side training\n",
    "        Args:\n",
    "            intermidiate_to_server (torch.Tensor): the output of client-side\n",
    "                                                   model\n",
    "        Returns:\n",
    "            outputs (torch.Tensor): outputs of server-side model\n",
    "        \"\"\"\n",
    "        self.intermidiate_to_server = intermidiate_to_server\n",
    "        outputs = self.server_model(intermidiate_to_server)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def server_backward(self):\n",
    "        self.grad_to_client = self.intermidiate_to_server.grad.clone()\n",
    "        return self.grad_to_client\n",
    "\n",
    "    def train(self):\n",
    "        self.server_model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.server_model.eval()\n",
    "\n",
    "\n",
    "class SplitNN(torch.nn.Module):\n",
    "    def __init__(self, client, server,\n",
    "                 client_optimizer, server_optimizer\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \"\"\"class that expresses the whole architecture of SplitNN\n",
    "        Args:\n",
    "            client (attack_splitnn.splitnn.Client):\n",
    "            server (attack_splitnn.splitnn.Server):\n",
    "            clietn_optimizer\n",
    "            server_optimizer\n",
    "        Attributes:\n",
    "            client (attack_splitnn.splitnn.Client):\n",
    "            server (attack_splitnn.splitnn.Server):\n",
    "            clietn_optimizer\n",
    "            server_optimizer\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.server = server\n",
    "        self.client_optimizer = client_optimizer\n",
    "        self.server_optimizer = server_optimizer\n",
    "        self.grad_to_client =None\n",
    "\n",
    "        self.intermidiate_to_server = None\n",
    "\n",
    "    def forward(self, inputs,labels):\n",
    "        # execute client - feed forward network\n",
    "        self.labels=labels\n",
    "        self.intermidiate_to_server = self.client(inputs)\n",
    "        # execute server - feed forward netwoek\n",
    "        outputs = self.server(self.intermidiate_to_server)\n",
    "        # grad_to_client = self.server.server_backward(self.intermidiate_to_server)\n",
    "        # grad_to_client = self.server.server_backward()\n",
    "\n",
    "        return outputs,self.intermidiate_to_server\n",
    "\n",
    "    def backward(self):\n",
    "        # execute server - back propagation\n",
    "        self.grad_to_client = self.server.server_backward()\n",
    "        # execute client - back propagation\n",
    "        # if model=='Marvell':\n",
    "        #   grad_to_client=KL_gradient_perturb_function_creator(self.label,grad_to_client)\n",
    "        \n",
    "        self.client.client_backward(self.grad_to_client)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        self.client_optimizer.zero_grad()\n",
    "        self.server_optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.client_optimizer.step()\n",
    "        self.server_optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        self.client.train()\n",
    "        self.server.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.client.eval()\n",
    "        self.server.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_and_leak(train_auc, test_auc, na_leak_auc,ma_leak_auc,median_leak_auc):\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].plot(train_auc, marker='', color='skyblue', linewidth=2,label=\"Training AUC\")\n",
    "    ax[0].plot(test_auc, marker='', color='olive', linewidth=2,label=\"Testing AUC\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"AUC\")\n",
    "    ax[1].set_title(\"Leak AUC\")\n",
    "    ax[1].plot(na_leak_auc, marker='', color='skyblue', linewidth=2, label=\"Norm Leak AUC\")\n",
    "    ax[1].plot(ma_leak_auc, marker='', color='olive', linewidth=2, label=\"Mean Leak AUC\")\n",
    "    ax[1].plot(median_leak_auc, marker='', color='yellow', linewidth=2, label=\"Median Leak AUC\")\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "def plot_pre_labels(prediction,labels):\n",
    "    data = pd.DataFrame(columns=['label', 'plot_gradient','y_hat'])\n",
    "    data['True Label'] = labels.reshape(1,-1)[0].detach().numpy()\n",
    "    data['plot_gradient'] = labels.detach().numpy()\n",
    "    data['prediction'] = prediction.detach().numpy()\n",
    "    plt.cla()\n",
    "    sns.color_palette('Set1')\n",
    "    #sns.histplot(data, x='plot_gradient',y='y_hat', hue='label')\n",
    "    sns.jointplot(data=data, y='True Label', x='prediction', hue='True Label')\n",
    "    plt.xlabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve_isotropic_covariance\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy\n",
    "\n",
    "OBJECTIVE_EPSILON = 1e-16\n",
    "CONVEX_EPSILON = 1e-20\n",
    "NUM_CANDIDATE = 1\n",
    "\n",
    "\n",
    "def symKL_objective(lam10, lam20, lam11, lam21, u, v, d, g):\n",
    "    if (lam21 + v) == 0.0 or (lam20 + u) == 0.0 or (lam11 + v) == 0.0 or (lam10 + u) == 0.0:\n",
    "        return float('inf')\n",
    "    objective = (d - 1) * (lam20 + u) / (lam21 + v) \\\n",
    "                + (d - 1) * (lam21 + v) / (lam20 + u) \\\n",
    "                + (lam10 + u + g) / (lam11 + v) \\\n",
    "                + (lam11 + v + g) / (lam10 + u)\n",
    "    return objective\n",
    "\n",
    "\n",
    "def symKL_objective_zero_uv(lam10, lam11, g):\n",
    "    objective = (lam10 + g) / lam11 \\\n",
    "                + (lam11 + g) / lam10\n",
    "    return objective\n",
    "\n",
    "\n",
    "def solve_isotropic_covariance(u, v, d, g, p, P,\n",
    "                               lam10_init=None, lam20_init=None,\n",
    "                               lam11_init=None, lam21_init=None):\n",
    "    \"\"\" return the solution to the optimization problem\n",
    "        Args:\n",
    "        u ([type]): [the coordinate variance of the negative examples]\n",
    "        v ([type]): [the coordinate variance of the positive examples]\n",
    "        d ([type]): [the dimension of activation to protect]\n",
    "        g ([type]): [squared 2-norm of g_0 - g_1, i.e. \\|g^{(0)} - g^{(1)}\\|_2^2]\n",
    "        P ([type]): [the power constraint value]\n",
    "    \"\"\"\n",
    "\n",
    "    if u == 0.0 and v == 0.0:\n",
    "        return solve_zero_uv(g=g, p=p, P=P)\n",
    "\n",
    "    ordering = [0, 1, 2]\n",
    "    random.shuffle(x=ordering)\n",
    "\n",
    "    solutions = []\n",
    "    if u <= v:\n",
    "        for i in range(NUM_CANDIDATE):\n",
    "            if i % 3 == ordering[0]:\n",
    "                # print('a')\n",
    "                if lam20_init:  # if we pass an initialization\n",
    "                    lam20 = lam20_init\n",
    "                    # print('here')\n",
    "                else:\n",
    "                    lam20 = random.random() * P / (1 - p) / d\n",
    "                lam10, lam11 = None, None\n",
    "                # print('lam21', lam21)\n",
    "            elif i % 3 == ordering[1]:\n",
    "                # print('b')\n",
    "                if lam11_init:\n",
    "                    lam11 = lam11_init\n",
    "                else:\n",
    "                    lam11 = random.random() * P / p\n",
    "                lam10, lam20 = None, None\n",
    "                # print('lam11', lam11)\n",
    "            else:\n",
    "                # print('c')\n",
    "                if lam10_init:\n",
    "                    lam10 = lam10_init\n",
    "                else:\n",
    "                    lam10 = random.random() * P / (1 - p)\n",
    "                lam11, lam20 = None, None\n",
    "                # print('lam10', lam10)\n",
    "\n",
    "            solutions.append(solve_small_neg(u=u, v=v, d=d, g=g, p=p, P=P, lam10=lam10, lam11=lam11, lam20=lam20))\n",
    "\n",
    "    else:\n",
    "        for i in range(NUM_CANDIDATE):\n",
    "            if i % 3 == ordering[0]:\n",
    "                if lam21_init:\n",
    "                    lam21 = lam21_init\n",
    "                else:\n",
    "                    lam21 = random.random() * P / p / d\n",
    "                lam10, lam11 = None, None\n",
    "                # print('lam21', lam21)\n",
    "            elif i % 3 == ordering[1]:\n",
    "                if lam11_init:\n",
    "                    lam11 = lam11_init\n",
    "                else:\n",
    "                    lam11 = random.random() * P / p\n",
    "                lam10, lam21 = None, None\n",
    "                # print('lam11', lam11)\n",
    "            else:\n",
    "                if lam10_init:\n",
    "                    lam10 = lam10_init\n",
    "                else:\n",
    "                    lam10 = random.random() * P / (1 - p)\n",
    "                lam11, lam21 = None, None\n",
    "                # print('lam10', lam10)\n",
    "\n",
    "            solutions.append(solve_small_pos(u=u, v=v, d=d, g=g, p=p, P=P, lam10=lam10, lam11=lam11, lam21=lam21))\n",
    "\n",
    "    # print(solutions)\n",
    "    lam10, lam20, lam11, lam21, objective = min(solutions, key=lambda x: x[-1])\n",
    "\n",
    "    # print('sum', p * lam11 + p*(d-1)*lam21 + (1-p) * lam10 + (1-p)*(d-1)*lam20)\n",
    "\n",
    "    return (lam10, lam20, lam11, lam21, objective)\n",
    "\n",
    "\n",
    "def solve_zero_uv(g, p, P):\n",
    "    C = P\n",
    "\n",
    "    E = math.sqrt((C + (1 - p) * g) / (C + p * g))\n",
    "    tau = max((P / (p)) / (E + (1 - p) /  (p)), 0.0)\n",
    "    # print('tau', tau)\n",
    "    if 0 <= tau and tau <= P / (1 - p):\n",
    "        # print('A')\n",
    "        lam10 = tau\n",
    "        lam11 = max(P /  (p) - (1 - p) * tau /  (p), 0.0)\n",
    "    else:\n",
    "        # print('B')\n",
    "        lam10_case1, lam11_case1 = 0.0, max(P /  (p), 0.0)\n",
    "        lam10_case2, lam11_case2 = max(P / (1 - p), 0), 0.0\n",
    "        objective1 = symKL_objective_zero_uv(lam10=lam10_case1, lam11=lam11_case1,\n",
    "                                             g=g)\n",
    "        objective2 = symKL_objective_zero_uv(lam10=lam10_case2, lam11=lam11_case2,\n",
    "                                             g=g)\n",
    "        if objective1 < objective2:\n",
    "            lam10, lam11 = lam10_case1, lam11_case1\n",
    "        else:\n",
    "            lam10, lam11 = lam10_case2, lam11_case2\n",
    "\n",
    "    objective = symKL_objective_zero_uv(lam10=lam10, lam11=lam11, g=g)\n",
    "    # here we subtract d = 1 because the distribution is essentially one-dimensional\n",
    "    return (lam10, 0.0, lam11, 0.0, 0.5 * objective - 1)\n",
    "\n",
    "\n",
    "def solve_small_neg(u, v, d, g, p, P, lam10=None, lam20=None, lam11=None):\n",
    "    \"\"\"[When u < v]\n",
    "    \"\"\"\n",
    "    # some intialization to start the alternating optimization\n",
    "    LAM21 = 0.0\n",
    "    i = 0\n",
    "    objective_value_list = []\n",
    "\n",
    "    if lam20:\n",
    "        ordering = [0, 1, 2]\n",
    "    elif lam11:\n",
    "        ordering = [1, 0, 2]\n",
    "    else:\n",
    "        ordering = [1, 2, 0]\n",
    "    # print(ordering)\n",
    "\n",
    "    while True:\n",
    "        if i % 3 == ordering[0]:  # fix lam20\n",
    "            D = P - (1 - p) * (d - 1) * lam20\n",
    "            C = D + p * v + (1 - p) * u\n",
    "\n",
    "            E = math.sqrt((C + (1 - p) * g) / (C + p * g))\n",
    "            tau = max((D / p + v - E * u) / (E + (1 - p) / p), 0.0)\n",
    "            # print('tau', tau)\n",
    "            if lam20 <= tau and tau <= P / (1 - p) - (d - 1) * lam20:\n",
    "                # print('A')\n",
    "                lam10 = tau\n",
    "                lam11 = max(D / p - (1 - p) * tau / p, 0.0)\n",
    "            else:\n",
    "                # print('B')\n",
    "                lam10_case1, lam11_case1 = lam20, max(P / p - (1 - p) * d * lam20 / p, 0.0)\n",
    "                lam10_case2, lam11_case2 = max(P / (1 - p) - (d - 1) * lam20, 0), 0.0\n",
    "                objective1 = symKL_objective(lam10=lam10_case1, lam20=lam20, lam11=lam11_case1, lam21=LAM21,\n",
    "                                             u=u, v=v, d=d, g=g)\n",
    "                objective2 = symKL_objective(lam10=lam10_case2, lam20=lam20, lam11=lam11_case2, lam21=LAM21,\n",
    "                                             u=u, v=v, d=d, g=g)\n",
    "                if objective1 < objective2:\n",
    "                    lam10, lam11 = lam10_case1, lam11_case1\n",
    "                else:\n",
    "                    lam10, lam11 = lam10_case2, lam11_case2\n",
    "\n",
    "        elif i % 3 == ordering[1]:  # fix lam11\n",
    "            D = max((P - p * lam11) / (1 - p), 0.0)\n",
    "            f = lambda x: symKL_objective(lam10=D - (d - 1) * x, lam20=x, lam11=lam11, lam21=LAM21,\n",
    "                                          u=u, v=v, d=d, g=g)\n",
    "\n",
    "            # f_prime = lambda x: (d-1)/v - (d-1)/(lam11+v) - (d-1)*v/((x+u)**2) + (lam11 + v + g)*(d-1)/((D-(d-1)*x+u)**2) # not numerically stable\n",
    "            # f_prime = lambda x: (d-1)/v - (d-1)/(lam11+v) - (d-1)/(x+u)*(v/(x+u)) + (lam11 + v + g)/(D-(d-1)*x+u) * ((d-1)/(D-(d-1)*x+u))\n",
    "\n",
    "            def f_prime(x):\n",
    "                if x == 0.0 and u == 0.0:\n",
    "                    return float('-inf')\n",
    "                else:\n",
    "                    return (d - 1) / v - (d - 1) / (lam11 + v) - (d - 1) / (x + u) * (v / (x + u)) + (lam11 + v + g) / (\n",
    "                                D - (d - 1) * x + u) * ((d - 1) / (D - (d - 1) * x + u))\n",
    "\n",
    "            # print('D/d', D/d)\n",
    "            lam20 = convex_min_1d(xl=0.0, xr=D / d, f=f, f_prime=f_prime)\n",
    "            lam10 = max(D - (d - 1) * lam20, 0.0)\n",
    "\n",
    "        else:  # fix lam10\n",
    "            D = max(P - (1 - p) * lam10, 0.0)  # avoid negative due to numerical error\n",
    "            f = lambda x: symKL_objective(lam10=lam10, lam20=x, lam11=D / p - (1 - p) * (d - 1) * x / p, lam21=LAM21,\n",
    "                                          u=u, v=v, d=d, g=g)\n",
    "\n",
    "            # f_prime = lambda x: (d-1)/v - (1-p)*(d-1)/(lam10 + u)/p - (d-1)*v/((x+u)**2) + (lam10+u+g)*(1-p)*(d-1)/p/((D/p - (1-p)*(d-1)*x/p + v)**2) # not numerically stable\n",
    "            # f_prime = lambda x: (d-1)/v - (1-p)*(d-1)/(lam10 + u)/p - (d-1)/(x+u)*(v/(x+u)) + (lam10+u+g)/(D/p - (1-p)*(d-1)*x/p + v) * (1-p) * (d-1) / p / (D/p - (1-p)*(d-1)*x/p + v)\n",
    "\n",
    "            def f_prime(x):\n",
    "                if x == 0.0 and u == 0.0:\n",
    "                    return float('-inf')\n",
    "                else:\n",
    "                    return (d - 1) / v - (1 - p) * (d - 1) / (lam10 + u) / p - (d - 1) / (x + u) * (v / (x + u)) + (\n",
    "                                lam10 + u + g) / (D / p - (1 - p) * (d - 1) * x / p + v) * (1 - p) * (d - 1) / p / (\n",
    "                                       D / p - (1 - p) * (d - 1) * x / p + v)\n",
    "\n",
    "            # print('lam10', 'D/((1-p)*(d-1)', lam10, D/((1-p)*(d-1)))\n",
    "            lam20 = convex_min_1d(xl=0.0, xr=min(D / ((1 - p) * (d - 1)), lam10), f=f, f_prime=f_prime)\n",
    "            lam11 = max(D / p - (1 - p) * (d - 1) * lam20 / p, 0.0)\n",
    "\n",
    "        if lam10 < 0 or lam20 < 0 or lam11 < 0 or LAM21 < 0:  # check to make sure no negative values\n",
    "            assert False, i\n",
    "\n",
    "        objective_value_list.append(symKL_objective(lam10=lam10, lam20=lam20, lam11=lam11, lam21=LAM21,\n",
    "                                                    u=u, v=v, d=d, g=g))\n",
    "        # print(i)\n",
    "        # print(objective_value_list[-1])\n",
    "        # print(lam10, lam20, lam11, LAM21, objective_value_list[-1])\n",
    "        # print('sum', p * lam11 + p*(d-1)*LAM21 + (1-p) * lam10 + (1-p)*(d-1)*lam20)\n",
    "\n",
    "        if (i >= 3 and objective_value_list[-4] - objective_value_list[-1] < OBJECTIVE_EPSILON) or i >= 100:\n",
    "            # print(i)\n",
    "            return lam10, lam20, lam11, LAM21, 0.5 * objective_value_list[-1] - d\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def solve_small_pos(u, v, d, g, p, P, lam10=None, lam11=None, lam21=None):\n",
    "    \"\"\"[When u > v] lam20 = 0.0 and will not change throughout the optimization\n",
    "    \"\"\"\n",
    "    # some intialization to start the alternating optimization\n",
    "    LAM20 = 0.0\n",
    "    i = 0\n",
    "    objective_value_list = []\n",
    "    if lam21:\n",
    "        ordering = [0, 1, 2]\n",
    "    elif lam11:\n",
    "        ordering = [1, 0, 2]\n",
    "    else:\n",
    "        ordering = [1, 2, 0]\n",
    "    # print(ordering)\n",
    "    while True:\n",
    "        if i % 3 == ordering[0]:  # fix lam21\n",
    "            D = P - p * (d - 1) * lam21\n",
    "            C = D + p * v + (1 - p) * u\n",
    "\n",
    "            E = math.sqrt((C + (1 - p) * g) / (C + p * g))\n",
    "            tau = max((D / p + v - E * u) / (E + (1 - p) / p), 0.0)\n",
    "            # print('tau', tau)\n",
    "            if 0.0 <= tau and tau <= (P - p * d * lam21) / (1 - p):\n",
    "                # print('A')\n",
    "                lam10 = tau\n",
    "                lam11 = max(D / (p) - (1 - p) * tau / (p), 0.0)\n",
    "            else:\n",
    "                # print('B')\n",
    "                lam10_case1, lam11_case1 = 0, max(P / p - (d - 1) * lam21, 0.0)\n",
    "                lam10_case2, lam11_case2 = max((P - p * d * lam21) / (1 - p), 0.0), lam21\n",
    "                objective1 = symKL_objective(lam10=lam10_case1, lam20=LAM20, lam11=lam11_case1, lam21=lam21,\n",
    "                                             u=u, v=v, d=d, g=g)\n",
    "                objective2 = symKL_objective(lam10=lam10_case2, lam20=LAM20, lam11=lam11_case2, lam21=lam21,\n",
    "                                             u=u, v=v, d=d, g=g)\n",
    "                if objective1 < objective2:\n",
    "                    lam10, lam11 = lam10_case1, lam11_case1\n",
    "                else:\n",
    "                    lam10, lam11 = lam10_case2, lam11_case2\n",
    "\n",
    "        elif i % 3 == ordering[1]:  # fix lam11\n",
    "            D = max(P - p * lam11, 0.0)\n",
    "            f = lambda x: symKL_objective(lam10=(D - p * (d - 1) * x) / (1 - p), lam20=LAM20, lam11=lam11, lam21=x,\n",
    "                                          u=u, v=v, d=d, g=g)\n",
    "\n",
    "            # f_prime = lambda x: (d-1)/u - p*(d-1)/(lam11+v)/(1-p) - (d-1)*u/((x+v)**2) + (lam11 + v + g)*p*(d-1)/(1-p)/(((D - p*(d-1)*x)/(1-p) + u)**2) # not numerically stable\n",
    "            # print('D', D)\n",
    "            # print('P', P)\n",
    "            # print('d', d)\n",
    "            # print('u', u)\n",
    "            # print('v', v)\n",
    "            # print('g', g)\n",
    "            # print('p', p)\n",
    "            # print('lam11', lam11)\n",
    "            # print()\n",
    "\n",
    "            # f_prime = lambda x: (d-1)/u - p*(d-1)/(lam11+v)/(1-p) - (d-1)/(x+v)*(u/(x+v)) + (lam11 + v + g) / ((D - p*(d-1)*x)/(1-p) + u) * p * (d-1) / (1-p) /((D - p*(d-1)*x)/(1-p) + u)\n",
    "\n",
    "            def f_prime(x):\n",
    "                if x == 0.0 and v == 0.0:\n",
    "                    return float('-inf')\n",
    "                else:\n",
    "                    return (d - 1) / u - p * (d - 1) / (lam11 + v) / (1 - p) - (d - 1) / (x + v) * (u / (x + v)) + (\n",
    "                                lam11 + v + g) / ((D - p * (d - 1) * x) / (1 - p) + u) * p * (d - 1) / (1 - p) / (\n",
    "                                       (D - p * (d - 1) * x) / (1 - p) + u)\n",
    "\n",
    "            # print('lam11', 'D/p/(d-1)', lam11, D/p/(d-1))\n",
    "            lam21 = convex_min_1d(xl=0.0, xr=min(D / p / (d - 1), lam11), f=f, f_prime=f_prime)\n",
    "            lam10 = max((D - p * (d - 1) * lam21) / (1 - p), 0.0)\n",
    "\n",
    "        else:  # fix lam10\n",
    "            D = max((P - (1 - p) * lam10) / p, 0.0)\n",
    "            f = lambda x: symKL_objective(lam10=lam10, lam20=LAM20, lam11=D - (d - 1) * x, lam21=x,\n",
    "                                          u=u, v=v, d=d, g=g)\n",
    "\n",
    "            # f_prime = lambda x: (d-1)/u - (d-1)/(lam10+u) - (d-1)*u/((x+v)**2) + (lam10 + u + g)*(d-1)/((D-(d-1)*x+v)**2)\n",
    "\n",
    "            # print('D', D)\n",
    "            # print('P', P)\n",
    "            # print('d', d)\n",
    "            # print('u', u)\n",
    "            # print('v', v)\n",
    "            # print('g', g)\n",
    "            # print('p', p)\n",
    "            # print('lam10', lam10)\n",
    "            # print()\n",
    "\n",
    "            # f_prime = lambda x: (d-1)/u - (d-1)/(lam10+u) - (d-1)/(x+v)*(u/(x+v)) + (lam10 + u + g)/(D-(d-1)*x+v) * (d-1) / (D-(d-1)*x+v)\n",
    "\n",
    "            def f_prime(x):\n",
    "                if x == 0.0 and v == 0.0:\n",
    "                    return float('-inf')\n",
    "                else:\n",
    "                    return (d - 1) / u - (d - 1) / (lam10 + u) - (d - 1) / (x + v) * (u / (x + v)) + (lam10 + u + g) / (\n",
    "                                D - (d - 1) * x + v) * (d - 1) / (D - (d - 1) * x + v)\n",
    "\n",
    "            # def f_prime(x):\n",
    "            #     print('x', x)\n",
    "            #     print('d, u, v, g', d, u, v, g)\n",
    "            #     print('(d-1)/u', (d-1)/u)\n",
    "            #     print('(d-1)/(lam10+u)', (d-1)/(lam10+u))\n",
    "            #     print('(d-1)*u/((x+v)**2)', (d-1)*u/((x+v)**2))\n",
    "            #     print('(lam10 + u + g)*(d-1)/((D-(d-1)*x+v)**2)', (lam10 + u + g)*(d-1)/((D-(d-1)*x+v)**2))\n",
    "\n",
    "            #     return (d-1)/u - (d-1)/(lam10+u) - (d-1)*u/((x+v)**2) + (lam10 + u + g)*(d-1)/((D-(d-1)*x+v)**2)\n",
    "            # print('D/d', D/d)\n",
    "            lam21 = convex_min_1d(xl=0.0, xr=D / d, f=f, f_prime=f_prime)\n",
    "            lam11 = max(D - (d - 1) * lam21, 0.0)\n",
    "\n",
    "        if lam10 < 0 or LAM20 < 0 or lam11 < 0 or lam21 < 0:\n",
    "            assert False, i\n",
    "\n",
    "        objective_value_list.append(symKL_objective(lam10=lam10, lam20=LAM20, lam11=lam11, lam21=lam21,\n",
    "                                                    u=u, v=v, d=d, g=g))\n",
    "        # print(i)\n",
    "        # print(objective_value_list[-1])\n",
    "        # print(lam10, LAM20, lam11, lam21)\n",
    "        # print('sum', p * lam11 + p*(d-1)*lam21 + (1-p) * lam10 + (1-p)*(d-1)*LAM20)\n",
    "\n",
    "        if (i >= 3 and objective_value_list[-4] - objective_value_list[-1] < OBJECTIVE_EPSILON) or i >= 100:\n",
    "            # print(i)\n",
    "            return lam10, LAM20, lam11, lam21, 0.5 * objective_value_list[-1] - d\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def convex_min_1d(xl, xr, f, f_prime):\n",
    "    # print('xl, xr', xl, xr)\n",
    "    assert xr <= 1e5\n",
    "    assert xl <= xr, (xl, xr)\n",
    "    # print('xl, xr', xl, xr)\n",
    "\n",
    "    xm = (xl + xr) / 2\n",
    "    # print('xl', xl, f(xl), f_prime(xl))\n",
    "    # print('xr', xr, f(xr), f_prime(xr))\n",
    "    # print('xm', xm, f(xm), f_prime(xm))\n",
    "    # print('abs(xl - xr) <= CONVEX_EPSILON',abs(xl - xr) <= CONVEX_EPSILON,abs(xl - xr) , CONVEX_EPSILON)\n",
    "    if abs(xl - xr) <= CONVEX_EPSILON:\n",
    "        # print('min((f(x), x) for x in [xl, xm, xr])[1]',min((f(x), x) for x in [xl, xm, xr])[1])\n",
    "        return min((f(x), x) for x in [xl, xm, xr])[1]\n",
    "    if f_prime(xl) <= 0 and f_prime(xr) <= 0:\n",
    "        return xr\n",
    "    elif f_prime(xl) >= 0 and f_prime(xr) >= 0:\n",
    "        return xl\n",
    "    if f_prime(xm) > 0:\n",
    "        # print('xm', xm, f(xm), f_prime(xm))\n",
    "        return convex_min_1d(xl=xl, xr=xm, f=f, f_prime=f_prime)\n",
    "    else:\n",
    "        # print('xm', xm, f(xm), f_prime(xm))\n",
    "        return convex_min_1d(xl=xm, xr=xr, f=f, f_prime=f_prime)\n",
    "\n",
    "\n",
    "def small_neg_problem_string(u, v, d, g, p, P):\n",
    "    return 'minimize ({2}-1)*(z + {0})/{1} + ({2}-1)*{1}/(z+{0})+(x+{0}+{3})/(y+{1}) + (y+{1}+{3})/(x+{0}) subject to x>=0, y>=0, z>=0, z<=x, {4}*y+(1-{4})*x+(1-{4})*({2}-1)*z={5}'.format(\n",
    "        u, v, d, g, p, P)\n",
    "\n",
    "\n",
    "def small_pos_problem_string(u, v, d, g, p, P):\n",
    "    return 'minimize ({2}-1)*{0}/(z+{1}) + ({2}-1)*(z + {1})/{0} + (x+{0}+{3})/(y+{1}) + (y+{1}+{3})/(x+{0}) subject to x>=0, y>=0, z>=0, z<=y, {4}*y+(1-{4})*x+{4}*({2}-1)*z={5}'.format(\n",
    "        u, v, d, g, p, P)\n",
    "\n",
    "\n",
    "def zero_uv_problem_string(g, p, P):\n",
    "    return 'minimize (x+{0})/y + (y+{0})/x subject to x>=0, y>=0, {1}*y+(1-{1})*x={2}'.format(g, p, P)\n",
    "\n",
    "def KL_gradient_perturb_function_creator(Y_Train,g,p_frac='pos_frac', dynamic=False, error_prob_lower_bound=None,\n",
    "                                         sumKL_threshold=None, init_scale=1.0, uv_choice='uv'):\n",
    "    # print('p_frac', p_frac)\n",
    "    # print('dynamic', dynamic)\n",
    "    if dynamic and (error_prob_lower_bound is not None):\n",
    "        '''\n",
    "        if using dynamic and error_prob_lower_bound is specified, we use it to \n",
    "        determine the sumKL_threshold and overwrite what is stored in it before.\n",
    "        '''\n",
    "        sumKL_threshold = (2 - 4 * error_prob_lower_bound) ** 2\n",
    "        # print('error_prob_lower_bound', error_prob_lower_bound)\n",
    "        # print('implied sumKL_threshold', sumKL_threshold)\n",
    "    # elif dynamic:\n",
    "    #     print('using sumKL_threshold', sumKL_threshold)\n",
    "\n",
    "    # print('init_scale', init_scale)\n",
    "    # print('uv_choice', uv_choice)\n",
    "\n",
    "    y = list(Y_Train.iloc[:,0])\n",
    "    pos, neg = [], []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            pos.append(i)\n",
    "        else:\n",
    "            neg.append(i)\n",
    "    # print('pos', pos)\n",
    "    pos_g = [g[i] for i in pos]\n",
    "\n",
    "    pos_g_mean =numpy.mean(pos_g)\n",
    "    pos_coordinate_var=numpy.var(pos_g)\n",
    "    neg_g =[g[i] for i in neg]\n",
    "    neg_g_mean =numpy.mean(neg_g)\n",
    "    neg_coordinate_var =numpy.var(neg_g)\n",
    "\n",
    "    avg_pos_coordinate_var = numpy.mean(pos_coordinate_var)\n",
    "    avg_neg_coordinate_var = numpy.mean(neg_coordinate_var)\n",
    "\n",
    "    g_diff = pos_g_mean - neg_g_mean\n",
    "    g_diff_norm = numpy.sqrt(g_diff**2)\n",
    "\n",
    "\n",
    "    if uv_choice == 'uv':\n",
    "        u = float(avg_neg_coordinate_var)\n",
    "        v = float(avg_pos_coordinate_var)\n",
    "        # if u == 0.0:\n",
    "        #     print('neg_g')\n",
    "        #     print(neg_g)\n",
    "        # if v == 0.0:\n",
    "        #     print('pos_g')\n",
    "        #     print(pos_g)\n",
    "\n",
    "    if uv_choice == 'same':\n",
    "          u = float(avg_neg_coordinate_var + avg_pos_coordinate_var) / 2.0\n",
    "          v = float(avg_neg_coordinate_var + avg_pos_coordinate_var) / 2.0\n",
    "    elif uv_choice == 'zero':\n",
    "          u, v = 0.0, 0.0\n",
    "\n",
    "    d = len(Y_Train)\n",
    "    if p_frac == 'pos_frac':\n",
    "          p = float(numpy.sum(y) / len(y))  # p is set as the fraction of positive in the batch\n",
    "    else:\n",
    "          p = float(p_frac)\n",
    "\n",
    "    scale = init_scale\n",
    "    lam10, lam20, lam11, lam21 = None, None, None, None\n",
    "    while True:\n",
    "       P = scale * g_diff_norm ** 2\n",
    "            # print('g_diff_norm ** 2', g_diff_norm ** 2)\n",
    "            # print('P', P)\n",
    "            # print('u, v, d, p', u, v, d, p)\n",
    "       lam10, lam20, lam11, lam21, sumKL = \\\n",
    "                    solve_isotropic_covariance(\n",
    "                        u=u,\n",
    "                        v=v,\n",
    "                        d=d,\n",
    "                        g=g_diff_norm ** 2,\n",
    "                        p=p,\n",
    "                        P=P,\n",
    "                        lam10_init=lam10,\n",
    "                        lam20_init=lam20,\n",
    "                        lam11_init=lam11,\n",
    "                        lam21_init=lam21)\n",
    "       if not dynamic or sumKL <= sumKL_threshold:break\n",
    "\n",
    "\n",
    "\n",
    "    perturbed_g = g\n",
    "    perturbed_g += numpy.multiply(numpy.random.normal(0,1,len(y)),\n",
    "                                                      y) * g_diff * (\n",
    "                                           numpy.sqrt(lam11 - lam21) / g_diff_norm)\n",
    "    # print(',g,perturbed_g,lam11 , lam21,g_diff',g,perturbed_g,lam11 , lam21,g_diff)\n",
    "\n",
    "    if lam21 > 0.0:\n",
    "      perturbed_g += numpy.random.normal(0,1,len(y)) * y * numpy.sqrt(\n",
    "                        lam21)\n",
    "      # print('0 perturbed_g lam21',lam21,perturbed_g)\n",
    "\n",
    "                    # negative examples add noise in g1 - g0\n",
    "    perturbed_g += numpy.multiply(numpy.random.normal(0,1,len(y)),\n",
    "                                                      [1-y[i] for i in range(len(y))]) * g_diff * (\n",
    "                                           numpy.sqrt(lam10 - lam20) / g_diff_norm)\n",
    "    #print('1 lam21,g_diff_norm,perturbed_g ,',lam21,g_diff_norm,perturbed_g)\n",
    "\n",
    "\n",
    "                # add spherical noise to negative examples\n",
    "    if lam20 > 0.0:\n",
    "          perturbed_g +=numpy.random.normal(0,1,len(y)) *[1-y[i] for i in range(len(y))] * numpy.sqrt(\n",
    "                        lam20)\n",
    "          # print('2 perturbed_g',perturbed_g)\n",
    "    return perturbed_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SplitNN\n",
    "import torch\n",
    "class Client_marvell(torch.nn.Module):\n",
    "    def __init__(self, client_model):\n",
    "        super().__init__()\n",
    "        \"\"\"class that expresses the Client on SplitNN\n",
    "        Args:\n",
    "            client_model (torch model): client-side model\n",
    "        Attributes:\n",
    "            client_model (torch model): cliet-side model\n",
    "            client_side_intermidiate (torch.Tensor): output of\n",
    "                                                     client-side model\n",
    "            grad_from_server\n",
    "        \"\"\"\n",
    "\n",
    "        self.client_model = client_model\n",
    "        self.client_side_intermidiate = None\n",
    "        self.grad_from_server = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"client-side feed forward network\n",
    "        Args:\n",
    "            inputs (torch.Tensor): the input data\n",
    "        Returns:\n",
    "            intermidiate_to_server (torch.Tensor): the output of client-side\n",
    "                                                   model which the client sent\n",
    "                                                   to the server\n",
    "        \"\"\"\n",
    "\n",
    "        self.client_side_intermidiate = self.client_model(inputs)\n",
    "        # send intermidiate tensor to the server\n",
    "        intermidiate_to_server = self.client_side_intermidiate.detach()\\\n",
    "            .requires_grad_()\n",
    "\n",
    "        return intermidiate_to_server\n",
    "\n",
    "    def client_backward(self, grad_from_server):\n",
    "        \"\"\"client-side back propagation\n",
    "        Args:\n",
    "            grad_from_server: gradient which the server send to the client\n",
    "        \"\"\"\n",
    "        self.grad_from_server = grad_from_server\n",
    "        self.client_side_intermidiate.backward(grad_from_server)\n",
    "\n",
    "    def train(self):\n",
    "        self.client_model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.client_model.eval()\n",
    "\n",
    "\n",
    "class Server_marvell(torch.nn.Module):\n",
    "    def __init__(self, server_model):\n",
    "        super().__init__()\n",
    "        \"\"\"class that expresses the Server on SplitNN\n",
    "        Args:\n",
    "            server_model (torch model): server-side model\n",
    "        Attributes:\n",
    "            server_model (torch model): server-side model\n",
    "            intermidiate_to_server:\n",
    "            grad_to_client\n",
    "        \"\"\"\n",
    "        self.server_model = server_model\n",
    "\n",
    "        self.intermidiate_to_server = None\n",
    "        self.grad_to_client = None\n",
    "\n",
    "    def forward(self, intermidiate_to_server):\n",
    "        \"\"\"server-side training\n",
    "        Args:\n",
    "            intermidiate_to_server (torch.Tensor): the output of client-side\n",
    "                                                   model\n",
    "        Returns:\n",
    "            outputs (torch.Tensor): outputs of server-side model\n",
    "        \"\"\"\n",
    "        self.intermidiate_to_server = intermidiate_to_server\n",
    "        # print('intermidiate_to_server',intermidiate_to_server)\n",
    "        outputs = self.server_model(intermidiate_to_server)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def server_backward(self):\n",
    "        self.grad_to_client = self.intermidiate_to_server.grad.clone()\n",
    "        return self.grad_to_client\n",
    "\n",
    "    def train(self):\n",
    "        self.server_model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.server_model.eval()\n",
    "\n",
    "\n",
    "class SplitNN_marvell(torch.nn.Module):\n",
    "    def __init__(self, client, server,\n",
    "                 client_optimizer, server_optimizer,init_scale\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \"\"\"class that expresses the whole architecture of SplitNN\n",
    "        Args:\n",
    "            client (attack_splitnn.splitnn.Client):\n",
    "            server (attack_splitnn.splitnn.Server):\n",
    "            clietn_optimizer\n",
    "            server_optimizer\n",
    "        Attributes:\n",
    "            client (attack_splitnn.splitnn.Client):\n",
    "            server (attack_splitnn.splitnn.Server):\n",
    "            clietn_optimizer\n",
    "            server_optimizer\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.server = server\n",
    "        self.client_optimizer = client_optimizer\n",
    "        self.server_optimizer = server_optimizer\n",
    "        self.grad_to_client=None\n",
    "        self.grad_to_client_1=None\n",
    "        self.init_scale = init_scale\n",
    "\n",
    "        self.intermidiate_to_server = None\n",
    "\n",
    "    def forward(self, inputs,labels):\n",
    "        # execute client - feed forward network\n",
    "        self.intermidiate_to_server = self.client(inputs)\n",
    "        # execute server - feed forward netwoek\n",
    "        # g_o=self.intermidiate_to_server1.grad.clone()\n",
    "        outputs = self.server(self.intermidiate_to_server)\n",
    "        self.labels=pd.DataFrame(labels.cpu().detach().numpy())\n",
    "    \n",
    "\n",
    "        return outputs,self.intermidiate_to_server\n",
    "\n",
    "    def backward(self):\n",
    "        # execute server - back propagation\n",
    "        self.grad_to_client_1 = self.server.server_backward()\n",
    "        # print(' self.grad_to_client_1',self.grad_to_client_1)\n",
    "\n",
    "       \n",
    "        self.grad_to_client=KL_gradient_perturb_function_creator(self.labels,\n",
    "                                                            self.grad_to_client_1.cpu().detach().numpy().T[0],\n",
    "                                              dynamic=False, error_prob_lower_bound=None,\n",
    "                                         sumKL_threshold=None, init_scale=self.init_scale, uv_choice='uv')\n",
    "        # print(' self.grad_to_client',self.grad_to_client)\n",
    "        # print('grad_to_client',grad_to_client)\n",
    "        self.grad_to_client=torch.Tensor(self.grad_to_client).reshape(-1,1)\n",
    "        self.grad_to_client = self.grad_to_client.to(device)\n",
    "        # execute client - back propagation\n",
    "        # if model=='Marvell':\n",
    "        # print('grad_to_client before',grad_to_client.T)\n",
    "        # print('grad_to_client.detach().numpy().T[0]',grad_to_client.detach().numpy().T[0])\n",
    "      \n",
    "        # print('grad_to_client after',grad_to_client.T)\n",
    "        \n",
    "        self.client.client_backward(self.grad_to_client)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        self.client_optimizer.zero_grad()\n",
    "        self.server_optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.client_optimizer.step()\n",
    "        self.server_optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        self.client.train()\n",
    "        self.server.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.client.eval()\n",
    "        self.server.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_marvell(Epochs,lr = 1e-4,init_scale=1,info=True):\n",
    "  model_1 = FirstNet()\n",
    "  model_1 = model_1.to(device)\n",
    "\n",
    "  model_2 = SecondNet()\n",
    "  model_2 = model_2.to(device)\n",
    "\n",
    "  model_1.double()\n",
    "  model_2.double()\n",
    "\n",
    "  opt_1 = optim.Adam(model_1.parameters(), lr=lr)\n",
    "  opt_2 = optim.Adam(model_2.parameters(), lr=lr)\n",
    "\n",
    "  BCE = nn.BCELoss()\n",
    "\n",
    "  client = Client_marvell(model_1)\n",
    "  server = Server_marvell(model_2)\n",
    "\n",
    "  splitnn_marvell = SplitNN_marvell(client, server, opt_1, opt_2,init_scale)\n",
    "  splitnn_marvell.train()\n",
    "\n",
    "  for epoch in range(Epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_outputs = []\n",
    "    epoch_labels = []\n",
    "    epoch_outputs_test = []\n",
    "    epoch_labels_test = []\n",
    "    epoch_g_norm=[]\n",
    "    epoch_g_mean=[]\n",
    "    epoch_g_inner=[]\n",
    "    epoch_g=[]\n",
    "    for i, data in enumerate(train_loader):\n",
    "        splitnn_marvell.zero_grads()\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device).double()\n",
    "        labels = labels.to(device).double()\n",
    "        \n",
    "        outputs,intermidiate_to_server = splitnn_marvell(inputs,labels)\n",
    "        loss = BCE(outputs, labels)\n",
    "        loss.backward()\n",
    "        splitnn_marvell.backward()\n",
    "        splitnn_marvell.step()\n",
    "\n",
    "        epoch_loss += loss.item() / len(train_loader.dataset)\n",
    "        \n",
    "        epoch_outputs.append(outputs)\n",
    "        epoch_labels.append(labels)\n",
    "\n",
    "        grad_from_server =splitnn_marvell.client.grad_from_server\n",
    "        g=list(grad_from_server.cpu().detach().numpy())\n",
    "        g_norm = grad_from_server.pow(2).sum(dim=1).sqrt()\n",
    "        v_1=np.multiply(grad_from_server.cpu().detach().numpy(),labels.cpu().detach().numpy())\n",
    "        mean_1=v_1.sum()/len(v_1[v_1!=0])\n",
    "        mean_0=(grad_from_server.cpu().detach().numpy().sum()-\n",
    "               v_1.sum())/len(v_1[v_1==0])\n",
    "\n",
    "        \n",
    "        g_mean=[]\n",
    "        for a in g:\n",
    "          if (a-mean_1)**2<(a-mean_0)**2:g_mean.append([1])\n",
    "          else:g_mean.append([0])\n",
    "        g_mean=torch.tensor(g_mean)\n",
    "        g_inner=[]\n",
    "        g=list(grad_from_server.cpu().detach().numpy())\n",
    "        g_inner=[]\n",
    "        for a in g:\n",
    "          if a>grad_from_server.median().item():g_inner.append(1)\n",
    "          else:g_inner.append(0)\n",
    "        g_inner=torch.tensor(g_inner)\n",
    "           \n",
    "        epoch_g_norm.append(g_norm)\n",
    "        epoch_g_mean.append(g_mean)\n",
    "        epoch_g_inner.append(g_inner)\n",
    "        epoch_g.append(grad_from_server)\n",
    "\n",
    "        t=next(iter(test_loader))\n",
    "        outputs_test,_ = splitnn_marvell(t[0].to(device),t[1].to(device))\n",
    "        labels_test=t[1]\n",
    "        epoch_outputs_test.append(outputs_test)\n",
    "        epoch_labels_test.append(labels_test)\n",
    "           \n",
    "        # print('labels',torch.cat(epoch_g_norm).shape)\n",
    "        # print('epoch_g_norm',torch.cat(epoch_g_norm).shape)\n",
    "        # print('epoch_g_norm',torch.cat(epoch_g_norm).shape)\n",
    "\n",
    "    # print(intermidiate_gradients)\n",
    "    # print(epoch_outputs)\n",
    "    # print('epoch_g_norm',torch.cat(epoch_g_norm).shape)\n",
    "    # print('epoch_g_mean',torch.cat(epoch_g_mean).shape)\n",
    "    # print('epoch_labels',torch.cat(epoch_labels).shape)\n",
    "\n",
    "  \n",
    "   \n",
    "    train_auc=torch_auc(torch.cat(epoch_labels),\n",
    "                                torch.cat(epoch_outputs))\n",
    "    test_auc=torch_auc(torch.cat(epoch_labels_test),\n",
    "                                torch.cat(epoch_outputs_test))\n",
    "    train_tvd=0\n",
    "    na_leak_auc=max(torch_auc(torch.cat(epoch_labels), torch.cat(epoch_g_norm).view(-1, 1)),\n",
    "                                      1-torch_auc(torch.cat(epoch_labels), \n",
    "                                                  torch.cat(epoch_g_norm).view(-1, 1)))\n",
    "    ma_leak_auc=max(torch_auc(torch.cat(epoch_labels), torch.cat(epoch_g_mean).view(-1, 1)),\n",
    "                                      1-torch_auc(torch.cat(epoch_labels), \n",
    "                                                  torch.cat(epoch_g_mean).view(-1, 1)))\n",
    "    cos_leak_auc=max(torch_auc(torch.cat(epoch_labels), torch.cat(epoch_g_inner).view(-1, 1)),\n",
    "                                      1-torch_auc(torch.cat(epoch_labels), \n",
    "                                                  torch.cat(epoch_g_inner).view(-1, 1)))\n",
    "    if info==True and (epoch%10==0 or epoch==Epochs-1):\n",
    "      print('Epoch',epoch,'Training Loss',epoch_loss, \n",
    "          'Training AUC',train_auc,\n",
    "             'Testing AUC',test_auc,\n",
    "            \"TVD\",train_tvd,\n",
    "            'NA Leak AUC',na_leak_auc,\n",
    "          'MA Leak AUC',ma_leak_auc,\n",
    "          'Median Leak AUC',cos_leak_auc\n",
    "          )\n",
    "  return train_auc,test_auc,train_tvd,na_leak_auc,ma_leak_auc,cos_leak_auc,splitnn_marvell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Marvell with different seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss 0.0027035087819332654 Training AUC 0.593296604429708 Testing AUC 0.6101009092006382 TVD 0 NA Leak AUC 0.5513755142888913 MA Leak AUC 0.6928621250871891 Median Leak AUC 0.6865147640083702\n",
      "Epoch 10 Training Loss 0.002704580776869042 Training AUC 0.602326253752919 Testing AUC 0.6052194630568635 TVD 0 NA Leak AUC 0.5533249092728688 MA Leak AUC 0.6976052080911416 Median Leak AUC 0.6930248779353638\n",
      "Epoch 20 Training Loss 0.0027045369930593366 Training AUC 0.6115002577762503 Testing AUC 0.603371771987627 TVD 0 NA Leak AUC 0.5463813268906119 MA Leak AUC 0.7015345268542199 Median Leak AUC 0.6988839804696582\n",
      "Epoch 30 Training Loss 0.002705956143624192 Training AUC 0.6211218826764251 Testing AUC 0.6348327491784914 TVD 0 NA Leak AUC 0.5494281410794255 MA Leak AUC 0.700418507323878 Median Leak AUC 0.6995349918623576\n",
      "Epoch 40 Training Loss 0.002702846873044507 Training AUC 0.6299922161681308 Testing AUC 0.6371564695685199 TVD 0 NA Leak AUC 0.5298031802513066 MA Leak AUC 0.6773076028830505 Median Leak AUC 0.6754475703324808\n",
      "Epoch 50 Training Loss 0.0027069960637116485 Training AUC 0.638707075199903 Testing AUC 0.6502964637984678 TVD 0 NA Leak AUC 0.5502404900781416 MA Leak AUC 0.6983259707044873 Median Leak AUC 0.694977912113462\n",
      "Epoch 60 Training Loss 0.0026996120493281697 Training AUC 0.6472197567805262 Testing AUC 0.6575510258298102 TVD 0 NA Leak AUC 0.5335705548760147 MA Leak AUC 0.6988142292490118 Median Leak AUC 0.69693094629156\n",
      "Epoch 70 Training Loss 0.0026996761744163167 Training AUC 0.6559004478230542 Testing AUC 0.6749985859121929 TVD 0 NA Leak AUC 0.5378789563599972 MA Leak AUC 0.6982794698907231 Median Leak AUC 0.6923738665426644\n",
      "Epoch 80 Training Loss 0.002699836422252179 Training AUC 0.6645326162773066 Testing AUC 0.6747313595330453 TVD 0 NA Leak AUC 0.5528328093567725 MA Leak AUC 0.7083468960706812 Median Leak AUC 0.7040920716112532\n",
      "Epoch 90 Training Loss 0.002700206252260944 Training AUC 0.673121518757013 Testing AUC 0.6900011231761152 TVD 0 NA Leak AUC 0.5290769588467799 MA Leak AUC 0.6974192048360848 Median Leak AUC 0.6943269007207626\n",
      "Epoch 100 Training Loss 0.0026994594542875 Training AUC 0.681048492261658 Testing AUC 0.6750960107842109 TVD 0 NA Leak AUC 0.5297659796002951 MA Leak AUC 0.709462915601023 Median Leak AUC 0.7151592652871426\n",
      "Epoch 110 Training Loss 0.002697914202719969 Training AUC 0.6889394781799986 Testing AUC 0.700447085498063 TVD 0 NA Leak AUC 0.554053152451907 MA Leak AUC 0.6979772146012555 Median Leak AUC 0.6904208323645664\n",
      "Epoch 120 Training Loss 0.0026999877407187247 Training AUC 0.6965478200216328 Testing AUC 0.7087280415214866 TVD 0 NA Leak AUC 0.5485777827198932 MA Leak AUC 0.6953964194373402 Median Leak AUC 0.6878167867937689\n",
      "Epoch 130 Training Loss 0.002700900600628004 Training AUC 0.7039535800572162 Testing AUC 0.7153974696444789 TVD 0 NA Leak AUC 0.56647250892108 MA Leak AUC 0.6964194373401534 Median Leak AUC 0.689769820971867\n",
      "Epoch 140 Training Loss 0.002699762750706169 Training AUC 0.7106820456314508 Testing AUC 0.7173059747097259 TVD 0 NA Leak AUC 0.5289326041466595 MA Leak AUC 0.6978144617530807 Median Leak AUC 0.6923738665426644\n",
      "Epoch 150 Training Loss 0.002695918856579148 Training AUC 0.7176442283392134 Testing AUC 0.7365672983376337 TVD 0 NA Leak AUC 0.5340962162490017 MA Leak AUC 0.6966054405952105 Median Leak AUC 0.6956289235061613\n",
      "Epoch 160 Training Loss 0.002694696965499184 Training AUC 0.724416364242896 Testing AUC 0.7431421965682279 TVD 0 NA Leak AUC 0.5421307481576579 MA Leak AUC 0.7031853057428505 Median Leak AUC 0.6995349918623576\n",
      "Epoch 170 Training Loss 0.0026952109367149057 Training AUC 0.7305722632754769 Testing AUC 0.7360879384194181 TVD 0 NA Leak AUC 0.5473420741384714 MA Leak AUC 0.7070913740990467 Median Leak AUC 0.7047430830039525\n",
      "Epoch 180 Training Loss 0.0026948270912306666 Training AUC 0.7370872294612982 Testing AUC 0.7431479557127159 TVD 0 NA Leak AUC 0.5575382873548113 MA Leak AUC 0.7122762148337596 Median Leak AUC 0.7145082538944432\n",
      "Epoch 190 Training Loss 0.0026931118603692374 Training AUC 0.742990406679943 Testing AUC 0.7651431834524767 TVD 0 NA Leak AUC 0.5401538570403243 MA Leak AUC 0.7053940943966519 Median Leak AUC 0.6988839804696582\n",
      "Epoch 200 Training Loss 0.0026930943314680453 Training AUC 0.7486794779778211 Testing AUC 0.760266660910687 TVD 0 NA Leak AUC 0.5308476289639417 MA Leak AUC 0.7079748895605673 Median Leak AUC 0.7066961171820507\n",
      "Epoch 210 Training Loss 0.002694200156236243 Training AUC 0.754517756234647 Testing AUC 0.7723024067448712 TVD 0 NA Leak AUC 0.5425876691972544 MA Leak AUC 0.6930713787491282 Median Leak AUC 0.6975819576842595\n",
      "Epoch 220 Training Loss 0.00269472234561637 Training AUC 0.75968500753111 Testing AUC 0.7684915871607962 TVD 0 NA Leak AUC 0.5567271514208021 MA Leak AUC 0.6932573820041851 Median Leak AUC 0.689769820971867\n",
      "Epoch 230 Training Loss 0.0026943312390873657 Training AUC 0.7645380750684876 Testing AUC 0.772030755199562 TVD 0 NA Leak AUC 0.5424089443304387 MA Leak AUC 0.7172053010927691 Median Leak AUC 0.7079981399674494\n",
      "Epoch 240 Training Loss 0.0026944644883132363 Training AUC 0.769807628155232 Testing AUC 0.7782108313189853 TVD 0 NA Leak AUC 0.5491264923223113 MA Leak AUC 0.710625435945129 Median Leak AUC 0.7106021855382469\n",
      "Epoch 250 Training Loss 0.002689436496505229 Training AUC 0.7745947858435349 Testing AUC 0.7985568811989802 TVD 0 NA Leak AUC 0.5356080992286929 MA Leak AUC 0.703487561032318 Median Leak AUC 0.6995349918623576\n",
      "Epoch 260 Training Loss 0.0026977937682964004 Training AUC 0.7790932341315973 Testing AUC 0.8045368944572475 TVD 0 NA Leak AUC 0.5655008440908585 MA Leak AUC 0.7039293187630783 Median Leak AUC 0.6988839804696582\n",
      "Epoch 270 Training Loss 0.002692494795395064 Training AUC 0.7832491938174135 Testing AUC 0.7957811275684475 TVD 0 NA Leak AUC 0.5679577044772196 MA Leak AUC 0.7324343176005579 Median Leak AUC 0.7223203906068356\n",
      "Epoch 280 Training Loss 0.002695030503488714 Training AUC 0.7871762886285292 Testing AUC 0.8186360316772029 TVD 0 NA Leak AUC 0.5498753576013666 MA Leak AUC 0.7175075563822366 Median Leak AUC 0.7099511741455475\n",
      "Epoch 290 Training Loss 0.0026934799662454174 Training AUC 0.7912000242612941 Testing AUC 0.8107753047611028 TVD 0 NA Leak AUC 0.5432285717173965 MA Leak AUC 0.7185305742850499 Median Leak AUC 0.7164612880725413\n",
      "Epoch 299 Training Loss 0.0026898483260984723 Training AUC 0.7947019398926438 Testing AUC 0.8226798592948894 TVD 0 NA Leak AUC 0.5373476340183779 MA Leak AUC 0.6882817949314114 Median Leak AUC 0.6904208323645664\n",
      "Mean Training AUC 0.7947019398926438 0.0\n",
      "Mean Testing AUC 0.8226798592948894 0.0\n",
      "Mean TVD 0.0 0.0\n",
      "Mean NA Leak AUC 0.5373476340183779 0.0\n",
      "Mean MA Leak AUC 0.6882817949314114 0.0\n",
      "Mean Median Leak AUC 0.6904208323645664 0.0\n"
     ]
    }
   ],
   "source": [
    "train_auc_list_marvell,test_auc_list_marvell,train_tvd_list_marvell,na_leak_auc_list_marvell,ma_leak_auc_list_marvell,cos_leak_auc_list_marvell=[],[],[],[],[],[]\n",
    "best=1\n",
    "init_scale = 1\n",
    "Epoch = 300\n",
    "random.seed(6)\n",
    "for i in range(1):\n",
    "  train_auc_marvell,test_auc_marvell,train_tvd_marvell,na_leak_auc_marvell,ma_leak_auc_marvell,cos_leak_auc_marvell,splitnn_marvell=train_marvell(Epochs=Epoch,init_scale=init_scale,info=True)\n",
    "  \n",
    "  # train_auc_marvell,test_auc_marvell,na_leak_auc_marvell,ma_leak_auc_marvell,cos_leak_auc_marvell,splitnn_marvell=train_marvell(Epochs=300,info=True)\n",
    "  train_auc_list_marvell.append(train_auc_marvell)\n",
    "  test_auc_list_marvell.append(test_auc_marvell)\n",
    "  train_tvd_list_marvell.append(train_tvd_marvell)\n",
    "  na_leak_auc_list_marvell.append(na_leak_auc_marvell)\n",
    "  ma_leak_auc_list_marvell.append(ma_leak_auc_marvell)\n",
    "  cos_leak_auc_list_marvell.append(cos_leak_auc_marvell)\n",
    "  if na_leak_auc_marvell<best:\n",
    "    best=na_leak_auc_marvell\n",
    "    marvell_model=splitnn_marvell\n",
    "print('Mean Training AUC',np.mean(train_auc_list_marvell),np.std(train_auc_list_marvell))\n",
    "print('Mean Testing AUC',np.mean(test_auc_list_marvell),np.std(test_auc_list_marvell))\n",
    "print('Mean TVD',np.mean(train_tvd_list_marvell),np.std(train_tvd_list_marvell))\n",
    "print('Mean NA Leak AUC',np.mean(na_leak_auc_list_marvell),np.std(na_leak_auc_list_marvell))\n",
    "print('Mean MA Leak AUC',np.mean(ma_leak_auc_list_marvell),np.std(ma_leak_auc_list_marvell))\n",
    "print('Mean Median Leak AUC',np.mean(cos_leak_auc_list_marvell),np.std(cos_leak_auc_list_marvell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training AUC 0.7947019398926438 0.0\n",
      "Mean Testing AUC 0.8226798592948894 0.0\n",
      "Mean TVD 0.0 0.0\n",
      "Mean NA Leak AUC 0.5373476340183779 0.0\n",
      "Mean MA Leak AUC 0.6882817949314114 0.0\n",
      "Mean Median Leak AUC 0.6904208323645664 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean Training AUC',np.mean(train_auc_list_marvell),np.std(train_auc_list_marvell))\n",
    "print('Mean Testing AUC',np.mean(test_auc_list_marvell),np.std(test_auc_list_marvell))\n",
    "print('Mean TVD',np.mean(train_tvd_list_marvell),np.std(train_tvd_list_marvell))\n",
    "print('Mean NA Leak AUC',np.mean(na_leak_auc_list_marvell),np.std(na_leak_auc_list_marvell))\n",
    "print('Mean MA Leak AUC',np.mean(ma_leak_auc_list_marvell),np.std(ma_leak_auc_list_marvell))\n",
    "print('Mean Median Leak AUC',np.mean(cos_leak_auc_list_marvell),np.std(cos_leak_auc_list_marvell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exapmle 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss 0.003032412798307199 Training AUC 0.387173255966762 Testing AUC 0.3948984698984699 TVD 0 NA Leak AUC 0.5149267612183214 MA Leak AUC 0.6817949314112998 Median Leak AUC 0.6780516159032783\n",
      "Epoch 10 Training Loss 0.0030381953120327313 Training AUC 0.39303619987262817 Testing AUC 0.3808996433115641 TVD 0 NA Leak AUC 0.5212927226226458 MA Leak AUC 0.690165077888863 Median Leak AUC 0.6878167867937689\n",
      "Epoch 20 Training Loss 0.003031600536110635 Training AUC 0.3993853805485075 Testing AUC 0.4058447583460654 TVD 0 NA Leak AUC 0.5071459620108569 MA Leak AUC 0.6968611950709137 Median Leak AUC 0.6865147640083702\n",
      "Epoch 30 Training Loss 0.00303301541310979 Training AUC 0.40564762492039264 Testing AUC 0.3941555746710129 TVD 0 NA Leak AUC 0.5072616075129142 MA Leak AUC 0.6805394094396652 Median Leak AUC 0.6780516159032783\n",
      "Epoch 40 Training Loss 0.00302583701018108 Training AUC 0.4120728243179038 Testing AUC 0.40651166249505344 TVD 0 NA Leak AUC 0.5225660362099815 MA Leak AUC 0.7076726342710997 Median Leak AUC 0.7034410602185539\n",
      "Epoch 50 Training Loss 0.0030260530217271165 Training AUC 0.4187964376333107 Testing AUC 0.42163277401563853 TVD 0 NA Leak AUC 0.5198103575508224 MA Leak AUC 0.6953499186235758 Median Leak AUC 0.6917228551499651\n",
      "Epoch 60 Training Loss 0.0030256221375319827 Training AUC 0.42506717345814415 Testing AUC 0.41672140732076357 TVD 0 NA Leak AUC 0.5153177724088431 MA Leak AUC 0.6888630551034645 Median Leak AUC 0.6787026272959777\n",
      "Epoch 70 Training Loss 0.003022579671172199 Training AUC 0.43098612051797863 Testing AUC 0.4311994180473023 TVD 0 NA Leak AUC 0.5191807769679447 MA Leak AUC 0.6812369216461288 Median Leak AUC 0.6852127412229714\n",
      "Epoch 80 Training Loss 0.003026087271458692 Training AUC 0.43724715182515694 Testing AUC 0.43560812900819534 TVD 0 NA Leak AUC 0.5273471285747501 MA Leak AUC 0.6820739362938852 Median Leak AUC 0.6813066728667752\n",
      "Epoch 90 Training Loss 0.0030235176535112953 Training AUC 0.44362625476380624 Testing AUC 0.4288465199373252 TVD 0 NA Leak AUC 0.5303421853360695 MA Leak AUC 0.6918391071843758 Median Leak AUC 0.6917228551499651\n",
      "Epoch 100 Training Loss 0.0030210199538227376 Training AUC 0.4497857929905078 Testing AUC 0.461821494052313 TVD 0 NA Leak AUC 0.5157073683572071 MA Leak AUC 0.7062776098581725 Median Leak AUC 0.7027900488258545\n",
      "Epoch 110 Training Loss 0.0030183886428096153 Training AUC 0.45681388554734487 Testing AUC 0.4526872072120941 TVD 0 NA Leak AUC 0.5110601174650992 MA Leak AUC 0.6894908160892816 Median Leak AUC 0.6865147640083702\n",
      "Epoch 120 Training Loss 0.0030181893597419794 Training AUC 0.46318773187226425 Testing AUC 0.45931514220987907 TVD 0 NA Leak AUC 0.5195018347603693 MA Leak AUC 0.6842827249476866 Median Leak AUC 0.6832597070448733\n",
      "Epoch 130 Training Loss 0.0030169642635372402 Training AUC 0.4696723714404133 Testing AUC 0.47683399429158146 TVD 0 NA Leak AUC 0.5184638557261709 MA Leak AUC 0.6914671006742618 Median Leak AUC 0.6904208323645664\n",
      "Epoch 140 Training Loss 0.003013437674499486 Training AUC 0.47634544039303295 Testing AUC 0.48420563877725004 TVD 0 NA Leak AUC 0.5220909191997817 MA Leak AUC 0.6946756568239943 Median Leak AUC 0.6923738665426644\n",
      "Epoch 150 Training Loss 0.0030133603576094673 Training AUC 0.4827488046258201 Testing AUC 0.47369918309656706 TVD 0 NA Leak AUC 0.5324035866279834 MA Leak AUC 0.7022552894675657 Median Leak AUC 0.6988839804696582\n",
      "Epoch 160 Training Loss 0.003010768990312226 Training AUC 0.48952336665891655 Testing AUC 0.49414421743142256 TVD 0 NA Leak AUC 0.5099081103484528 MA Leak AUC 0.7009997674959312 Median Leak AUC 0.7001860032550569\n",
      "Epoch 170 Training Loss 0.003010580659713925 Training AUC 0.4959756578348817 Testing AUC 0.4933909615431862 TVD 0 NA Leak AUC 0.5278921989830474 MA Leak AUC 0.689397814461753 Median Leak AUC 0.6878167867937689\n",
      "Epoch 180 Training Loss 0.0030076468929782544 Training AUC 0.5031509355761552 Testing AUC 0.5102815300506802 TVD 0 NA Leak AUC 0.5137567603085228 MA Leak AUC 0.6733317833062079 Median Leak AUC 0.6695884677981865\n",
      "Epoch 190 Training Loss 0.003006495506319645 Training AUC 0.5100358864975789 Testing AUC 0.5036463248282036 TVD 0 NA Leak AUC 0.5263281542209597 MA Leak AUC 0.6874912810974192 Median Leak AUC 0.689769820971867\n",
      "Epoch 200 Training Loss 0.0030073020350553786 Training AUC 0.5171060319642551 Testing AUC 0.5158472061157227 TVD 0 NA Leak AUC 0.523331480040031 MA Leak AUC 0.6773541036968148 Median Leak AUC 0.6754475703324808\n",
      "Epoch 210 Training Loss 0.0030018123354750535 Training AUC 0.5239772348190006 Testing AUC 0.5158762797545274 TVD 0 NA Leak AUC 0.5110516260121509 MA Leak AUC 0.6809346663566613 Median Leak AUC 0.6787026272959777\n",
      "Epoch 220 Training Loss 0.0030048043161587405 Training AUC 0.5306725432912467 Testing AUC 0.5109717112032085 TVD 0 NA Leak AUC 0.5254270493211892 MA Leak AUC 0.6793071378749127 Median Leak AUC 0.6747965589397815\n",
      "Epoch 230 Training Loss 0.002998332135741785 Training AUC 0.5377426887579229 Testing AUC 0.5421517237428466 TVD 0 NA Leak AUC 0.5128101654822438 MA Leak AUC 0.6891885607998139 Median Leak AUC 0.689769820971867\n",
      "Epoch 240 Training Loss 0.0029991151891122863 Training AUC 0.5448690395560183 Testing AUC 0.5323740912269591 TVD 0 NA Leak AUC 0.5037279500217341 MA Leak AUC 0.6845849802371542 Median Leak AUC 0.6813066728667752\n",
      "Epoch 250 Training Loss 0.002997292488314982 Training AUC 0.551497225114483 Testing AUC 0.5542521431782833 TVD 0 NA Leak AUC 0.5079017013232514 MA Leak AUC 0.6849104859335039 Median Leak AUC 0.6813066728667752\n",
      "Epoch 260 Training Loss 0.00299746395455266 Training AUC 0.5587287081871759 Testing AUC 0.5489129529236589 TVD 0 NA Leak AUC 0.5216760510700241 MA Leak AUC 0.6827016972797024 Median Leak AUC 0.6819576842594746\n",
      "Epoch 270 Training Loss 0.0029954044039529463 Training AUC 0.5657610464704871 Testing AUC 0.5577319773800445 TVD 0 NA Leak AUC 0.5119545505089818 MA Leak AUC 0.699627993489886 Median Leak AUC 0.6962799348988606\n",
      "Epoch 280 Training Loss 0.0029950121028235986 Training AUC 0.5728968996087866 Testing AUC 0.5761489164589362 TVD 0 NA Leak AUC 0.5072587770285981 MA Leak AUC 0.6999534991862357 Median Leak AUC 0.6975819576842595\n",
      "Epoch 290 Training Loss 0.002992976342538161 Training AUC 0.5791247738139765 Testing AUC 0.582937431058397 TVD 0 NA Leak AUC 0.5253767071358532 MA Leak AUC 0.6997209951174146 Median Leak AUC 0.694977912113462\n",
      "Epoch 299 Training Loss 0.0029945027662549193 Training AUC 0.5853118081740343 Testing AUC 0.603217949885456 TVD 0 NA Leak AUC 0.5096541754698098 MA Leak AUC 0.6786793768890955 Median Leak AUC 0.6754475703324808\n",
      "Mean Training AUC 0.5853118081740343 0.0\n",
      "Mean Testing AUC 0.603217949885456 0.0\n",
      "Mean TVD 0.0 0.0\n",
      "Mean NA Leak AUC 0.5096541754698098 0.0\n",
      "Mean MA Leak AUC 0.6786793768890955 0.0\n",
      "Mean Median Leak AUC 0.6754475703324808 0.0\n"
     ]
    }
   ],
   "source": [
    "train_auc_list_marvell,test_auc_list_marvell,train_tvd_list_marvell,na_leak_auc_list_marvell,ma_leak_auc_list_marvell,cos_leak_auc_list_marvell=[],[],[],[],[],[]\n",
    "best=1\n",
    "init_scale = 1\n",
    "Epoch = 300\n",
    "random.seed(9)\n",
    "for i in range(1):\n",
    "  train_auc_marvell,test_auc_marvell,train_tvd_marvell,na_leak_auc_marvell,ma_leak_auc_marvell,cos_leak_auc_marvell,splitnn_marvell=train_marvell(Epochs=Epoch,init_scale=init_scale,info=True)\n",
    "  \n",
    "  # train_auc_marvell,test_auc_marvell,na_leak_auc_marvell,ma_leak_auc_marvell,cos_leak_auc_marvell,splitnn_marvell=train_marvell(Epochs=300,info=True)\n",
    "  train_auc_list_marvell.append(train_auc_marvell)\n",
    "  test_auc_list_marvell.append(test_auc_marvell)\n",
    "  train_tvd_list_marvell.append(train_tvd_marvell)\n",
    "  na_leak_auc_list_marvell.append(na_leak_auc_marvell)\n",
    "  ma_leak_auc_list_marvell.append(ma_leak_auc_marvell)\n",
    "  cos_leak_auc_list_marvell.append(cos_leak_auc_marvell)\n",
    "  if na_leak_auc_marvell<best:\n",
    "    best=na_leak_auc_marvell\n",
    "    marvell_model=splitnn_marvell\n",
    "print('Mean Training AUC',np.mean(train_auc_list_marvell),np.std(train_auc_list_marvell))\n",
    "print('Mean Testing AUC',np.mean(test_auc_list_marvell),np.std(test_auc_list_marvell))\n",
    "print('Mean TVD',np.mean(train_tvd_list_marvell),np.std(train_tvd_list_marvell))\n",
    "print('Mean NA Leak AUC',np.mean(na_leak_auc_list_marvell),np.std(na_leak_auc_list_marvell))\n",
    "print('Mean MA Leak AUC',np.mean(ma_leak_auc_list_marvell),np.std(ma_leak_auc_list_marvell))\n",
    "print('Mean Median Leak AUC',np.mean(cos_leak_auc_list_marvell),np.std(cos_leak_auc_list_marvell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training AUC 0.5853118081740343 0.0\n",
      "Mean Testing AUC 0.603217949885456 0.0\n",
      "Mean TVD 0.0 0.0\n",
      "Mean NA Leak AUC 0.5096541754698098 0.0\n",
      "Mean MA Leak AUC 0.6786793768890955 0.0\n",
      "Mean Median Leak AUC 0.6754475703324808 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Mean Training AUC',np.mean(train_auc_list_marvell),np.std(train_auc_list_marvell))\n",
    "print('Mean Testing AUC',np.mean(test_auc_list_marvell),np.std(test_auc_list_marvell))\n",
    "print('Mean TVD',np.mean(train_tvd_list_marvell),np.std(train_tvd_list_marvell))\n",
    "print('Mean NA Leak AUC',np.mean(na_leak_auc_list_marvell),np.std(na_leak_auc_list_marvell))\n",
    "print('Mean MA Leak AUC',np.mean(ma_leak_auc_list_marvell),np.std(ma_leak_auc_list_marvell))\n",
    "print('Mean Median Leak AUC',np.mean(cos_leak_auc_list_marvell),np.std(cos_leak_auc_list_marvell))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yjenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
